{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Q1**"
      ],
      "metadata": {
        "id": "wcrd5IIxkCy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO9pf0KPGO9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb784b24-9488-40e5-d3e5-0b7991f41419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u382-ga-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "ODx-yUgEoKg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "5xMIscyal0AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "C4Iu1uNGoTcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID of the files on Google Drive\n",
        "sales_data_id = '15uTTtFT-AkNYiH_DrmKoIBSq8PpO-e0Z'\n",
        "stores_data_id = '1zIp7B3Lm1F8HE5mhcfAh30kqQMx7ttQs'\n",
        "\n",
        "# Download 'sales-data-set.csv'\n",
        "sales_downloaded = drive.CreateFile({'id': sales_data_id})\n",
        "sales_downloaded.GetContentFile('sales-data-set.csv')\n",
        "\n",
        "# Download 'stores-data-set.csv'\n",
        "stores_downloaded = drive.CreateFile({'id': stores_data_id})\n",
        "stores_downloaded.GetContentFile('stores-data-set.csv')"
      ],
      "metadata": {
        "id": "CZc2KVN-oZI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "VdoKNjbUl5Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the sales data from 'sales-data-set.csv' into a DataFrame\n",
        "sales_data = spark.read.csv(\"sales-data-set.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Load the store data from 'stores-data-set.csv' into a DataFrame\n",
        "store_data = spark.read.csv(\"stores-data-set.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "GbEc2D5zmJyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a) The total sales for each store type. [10 pts (5 pts for code)] Note that the result should have 3 rows (header is not included)**"
      ],
      "metadata": {
        "id": "rYUBi00DjSxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the two DataFrames on the 'Store' column to associate sales with store types\n",
        "joined_data = sales_data.join(store_data, on=\"Store\")\n",
        "\n",
        "# Calculate the total sales for each store type and select only the relevant columns\n",
        "total_sales_by_type = joined_data.groupBy(\"Type\").agg(sum(\"Weekly_Sales\").alias(\"Total_Sales\"))\n",
        "\n",
        "# Reorder the rows to display \"A\" first, \"B\" second, and \"C\" third\n",
        "total_sales_by_type = total_sales_by_type.orderBy(col(\"Type\"))\n",
        "\n",
        "# Format the 'Total_Sales' column to display numbers with commas and two decimal places\n",
        "total_sales_by_type = total_sales_by_type.withColumn(\"Total_Sales\", format_number(\"Total_Sales\", 2))\n",
        "\n",
        "# Show the result (excluding the header)\n",
        "total_sales_by_type.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Len8EF3SnUBN",
        "outputId": "92b0186a-9d46-48db-ce5b-f3f5e80de4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------------+\n",
            "|Type|Total_Sales     |\n",
            "+----+----------------+\n",
            "|A   |4,331,014,722.75|\n",
            "|B   |2,000,700,736.82|\n",
            "|C   |405,503,527.54  |\n",
            "+----+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b) Average sales on Holidays vs. Non-Holidays, and determine if sales are generally higher during holidays. [10 pts (5 pts for code)] Note that the result should have 2 rows (header is not included)**"
      ],
      "metadata": {
        "id": "XyfXwFzTjyOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize another Spark session\n",
        "spark2 = SparkSession.builder.appName(\"HolidaySalesAnalysis\").getOrCreate()\n",
        "\n",
        "# Calculate average sales for holidays and non-holidays\n",
        "result = sales_data.groupBy(\"IsHoliday\")\\\n",
        "                  .agg(avg(\"Weekly_Sales\").alias(\"Avg_Sales\"))\\\n",
        "                  .withColumn(\"IsHoliday\", when(col(\"IsHoliday\") == \"false\", \"Non-Holiday\")\n",
        "                                        .otherwise(\"Holiday\"))\n",
        "\n",
        "# Show the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHdVaVy_j2ox",
        "outputId": "c1cc8504-0be5-4aeb-b104-b58ea055be14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|  IsHoliday|         Avg_Sales|\n",
            "+-----------+------------------+\n",
            "|    Holiday| 17035.82318735042|\n",
            "|Non-Holiday|15901.445069008514|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}